executors:
  hpc-cluster01:
    context:
      explore:
        anyware:
          # concurrency of the batch job
          concurrency: 5

          # script template to run batch job
          # for example:
          script_template:
            header: |
              #SBATCH -N 1
              #SBATCH --ntasks-per-node=4
              #SBATCH --job-name=explore
              #SBATCH --partition=c51-large
              #SBATCH --mem=8G
            setup: |
              set -e
              module load deepmd/2.2
              set +e


workflow:
  explore:
    # The most flexible way to explore the potential energy surface.
    # It allow you to use whatever software you want in the explore stage.
    anyware:
      # The system files to explore,
      # should be defined in the artifacts section
      system_files: [ h2o-64-explore ]

      # The format of the system file you want to use in the task
      # Support ase format like extxyz, lammps-data
      # Custome data format includes:
      # - cp2k-inc: cp2k include file, can be use in cp2k input via @include coord_n_cell.inc
      system_file_format: cp2k-inc

      # The name of the file you want to be used in the task
      system_file_name: coord_n_cell.inc

      # Define template files for each tasks.
      # The key is file name and the value is the content of the file.
      # You can use !load_text to load the content from a file,
      # or use | to write the content directly.
      #
      # You can reference template variables in the content via $$VAR_NAME
      # The variables are defined in product_vars, broadcast_vars.
      # Besides, there are some predefined variables, includes
      # - SYSTEM_FILE: the path of the system file
      # - DP_MODELS: the path of the deep potential models in the format of `1.pb 2.pb 3.pb 4.pb`
      #
      # For example:
      template_files:
        cp2k.inp: !load_text ./config/cp2k.inp

        lammps.in: |
          variable TEMP equal $$TEMP
          ...
          read $$SYSTEM_FILE
          ...
          pair style deepmd $$DP_MODELS

      # Specify the variants to explore.
      # Variants defined here will be used in full combination.
      # You can also specify custom variables here.
      # Each variable should be a list of values.
      product_vars:
        TEMP: [330, 430, 530]
        PRES: [1]

      # Optional, same as product_vars,
      # but the values will be broadcasted to all combinations.
      # Use this wisely when you have too many variants to explore to avoid combinatorial explosion.
      broadcast_vars:
        MY_VARS: [1, 2]

      # The command you want to run in the batch job in each task directory
      # Note that you must output 2 files in task folder
      # - structures.xyz: new structures generated by the task
      # - model_devi.out: model deviation of new structures
      #
      # If it is hard to generate the file in batch job,
      # you can defer this to the post_process_fn
      submit_script: |
        mpirun cp2k.popt -i cp2k.inp > cp2k.out

      # Optional, a python function that will be executed after the task is finished.
      # You may use this function to post-process the results.

      # The function must named as `post_process_fn` and accept a list of task directories as input.
      # The below is an example of merging multiple file into one by keeping only the last line of each file.
      post_process_fn: |
        def post_process_fn(task_dirs):
            import glob
            for task_dir in task_dirs:
                files = glob.glob(os.path.join(task_dir, '*.out'))  # file to merge
                with open(os.path.join(task_dir, 'merged.out'), 'w') as fp:
                    for file in files:
                        with open(file, 'r') as f:
                            lines = f.readlines()
                            if len(lines) > 0:
                                fp.write(lines[-1])

